{"cells":[{"cell_type":"markdown","metadata":{"id":"8S5mo9I3pA2Q"},"source":["# [NML-24] Assignment 2: Graph Neural Networks\n","\n","TAs: [William Cappelletti](https://people.epfl.ch/william.cappelletti) and [Abdellah Rahmani](https://people.epfl.ch/abdellah.rahmani)"]},{"cell_type":"markdown","metadata":{"id":"hl-xagi0AhNJ"},"source":["## Students\n","\n","* Team: `1`\n","* Students: `Nikolaos Efthymiou`, `Aristotelis Dimitriou`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount(\"/NML\")\n","\n","# %cd ../NML/MyDrive/NML_A2"]},{"cell_type":"markdown","metadata":{"id":"tlRBpSceAhNJ"},"source":["## Instructions\n","\n","**!! Read carefully before starting !!**\n","\n","**Deadline:** April 30\n","\n","**Grading:**\n","* The integrality of Assignment 2 will be scaled to 100% and will amount to 1/3 of the overall assignments score.\n","* The total number of points is **100**, the points for each exercise are stated in the instructions.\n","* All team members will receive the same grade based on the team solution.\n","* Collaboration between team members is encouraged. No collaboration between teams is allowed.\n","\n","**Expected output:**\n","\n","You will have coding and theoretical questions. Coding exercises shall be solved within the specified space:\n","```python\n","# Your solution here ###########################################################\n","...\n","#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","```\n","Sometimes we provide variable names, such as `x = ...`; do not change names and stick to hinted typing, as they will be reused later.\n","Within the solution space, you can declare any other variable or function that you might need, but anything outside these lines shall not be changed, or it will invalidate your answers.\n","\n","Theoretical questions shall be answered in the following markdown cell. The first line will be\n","```markdown\n","**Your answer here:**\n","...\n","```\n","\n","**Solutions:**\n","* Your submission is self-contained in the `.ipynb` file.\n","\n","* Code has to be clean and readable. Provide meaningful variable names and comment where needed.\n","\n","* Textual answers in [markdown cells][md_cells] shall be short: one to two\n","  sentences. Math shall be written in [LaTeX][md_latex].\n","    **NOTE**: handwritten notes pasted in the notebook are ignored.\n","\n","* You cannot import any other library than we imported, unless explicitly stated.\n","\n","* Make sure all cells are executed before submitting. I.e., if you open the notebook again it should show numerical results and plots. Cells not run are ignored.\n","\n","* Execute your notebook from a blank state before submission, to make sure it is reproducible. You can click \"Kernel\" then \"Restart Kernel and Run All Cells\" in Jupyter. We might re-run cells to ensure that the code is working and corresponds to the results.\n","\n","[md_cells]: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n","[md_latex]: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html#LaTex-equations"]},{"cell_type":"markdown","metadata":{"id":"cmp4-YzspA2V"},"source":["## Objective\n","\n","This assignment focuses on Graph Neural Networks. In the first part, you will load and prepare data using the PyTorch Geometric library. Next, you will define a GNN used to solve a task. You will train and test the Neural Network and comment on the results.\n","In the second part, you will define a new GNN block, in order to include it in the previous architecture.\n","The third part is theoretical and study a way to include structural properties in learned networks.\n","\n","## Prerequisites\n","\n","The additional [tutorial notebook](nml24_gnn_tutorial.ipynb) provides a broad overview of PyTorch and PyTorch Geometric, showing how to manipulate tensors and train neural networks and GNNs.\n","\n","The following resources might help you familiarize with PyTorch and PyTorch geometric.\n","\n","* [PyTorch: Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)\n","* [PyTorch geometric: Official tutorials](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html#official-examples)\n"]},{"cell_type":"markdown","metadata":{"id":"fv2QsxmdLWNN"},"source":["## Part 0: Explore the data [0 points]\n","\n","This part contains no questions, but we will go together through the data to get a feeling of their content.\n","We work with the [GitHub dataset](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.GitHub.html), from the [\"Multi-scale Attributed Node Embedding\"](https://arxiv.org/abs/1909.13021) paper.\n","In this dataset, nodes represent developers on GitHub and edges are mutual follower relationships. It contains 37,300 nodes, 578,006 edges, 128 node features and 2 classes.\n","\n","This data is readily available in PyTorch Geometric, let's start by installing it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131599,"status":"ok","timestamp":1713360185087,"user":{"displayName":"Nikolaos Efthymiou","userId":"04824858864620672700"},"user_tz":-120},"id":"21wTC9M3Vx1Z","outputId":"3041e3fc-c0a2-4aca-d35d-073ed62a0e3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install torch_geometric -q\n","!pip install pyg_lib -f https://data.pyg.org/whl/torch-2.2.0+cu121.html -q\n","!pip install torchmetrics -q"]},{"cell_type":"markdown","metadata":{"id":"ogDP1nNFGc8A"},"source":["Then, we can import all relevant libraries. Some of them will be useful in later steps."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-ZfCASHUyyK"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import networkx as nx\n","import numpy as np\n","import torch\n","import torch_geometric as pyg\n","from scipy import sparse\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","from torch import nn, optim\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch_geometric.datasets import GitHub\n","from torchmetrics import Metric\n","from torchmetrics.classification import Accuracy, BinaryF1Score, Precision, Recall"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iyzAIJkMRAR"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","metadata":{"id":"ZpKUE05sG5y_"},"source":["Let's download the data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4986,"status":"ok","timestamp":1713360507778,"user":{"displayName":"Nikolaos Efthymiou","userId":"04824858864620672700"},"user_tz":-120},"id":"PASnfjHmG5WN","outputId":"7664595b-a2a8-4977-d965-1648d683a88a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading https://graphmining.ai/datasets/ptg/github.npz\n","Processing...\n","Done!\n"]}],"source":["dataset = GitHub(\".\")\n","data = dataset._data"]},{"cell_type":"markdown","metadata":{"id":"l3YM75iqHExP"},"source":["Now, we shall study its content. Node attributes are accessible through the `x` attribute, which is a `torch.Tensor`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1713360541712,"user":{"displayName":"Nikolaos Efthymiou","userId":"04824858864620672700"},"user_tz":-120},"id":"EojTx0W9HEWB","outputId":"d3762a7e-8b4e-48ac-e5ce-49044c11d98c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Design matrix\n","Num. nodes: 37700; num features: 128\n"]}],"source":["print(\"Design matrix\")\n","n_nodes, n_feats = data.x.shape\n","print(f\"Num. nodes: {n_nodes}; num features: {n_feats}\")"]},{"cell_type":"markdown","metadata":{"id":"hkxKqIAlHcfn"},"source":["We see that we have 37,700 nodes, each with 128 features. The features correspond to an embedding of location, starred repositories, employer and e-mail address of each user.\n","\n","Each node comes with a 0/1 label, which indicates whether it corresponds to a web, or a machine learning developer."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1713360555373,"user":{"displayName":"Nikolaos Efthymiou","userId":"04824858864620672700"},"user_tz":-120},"id":"pADYpCsfILUX","outputId":"56dac913-1427-4613-9a34-50fc3a1720f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Target vector\n","First five elements: tensor([0, 0, 1, 0, 1])\n","Number of samples: torch.Size([37700])\n","Number of nodes in class 1: 9739\n"]}],"source":["print(\"Target vector\")\n","print(\"First five elements:\", data.y[:5])\n","print(\"Number of samples:\", data.y.shape)\n","print(\"Number of nodes in class 1:\", data.y.sum().item())"]},{"cell_type":"markdown","metadata":{"id":"sMWyuV57lN-v"},"source":["We see that the task is quite imbalanced, as class one is underrepresented. To get more meaningful interpretations, we swap classes zero and one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r96967dQJ909"},"outputs":[],"source":["data.y = 1 - data.y"]},{"cell_type":"markdown","metadata":{"id":"i8tddwFBIKls"},"source":["The edges are contained in the `edge_index` attribute, which is again a tensor. Let's check its shape."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":259,"status":"ok","timestamp":1713361077579,"user":{"displayName":"Nikolaos Efthymiou","userId":"04824858864620672700"},"user_tz":-120},"id":"GvC2cdChIKV_","outputId":"c3be1807-a73f-4489-e056-a6a3d42442ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Edge index shape: torch.Size([2, 578006])\n","tensor([[    0, 23977,     1, 34526,     1,  2370,     1, 14683],\n","        [23977,     0, 34526,     1,  2370,     1, 14683,     1]])\n","tensor([    0,     1,     2,  ..., 37697, 37698, 37699])\n"]}],"source":["print(\"Edge index shape:\", data.edge_index.shape)"]},{"cell_type":"markdown","metadata":{"id":"hIeGm9pZOIeU"},"source":["**0.1 [0 points]** Describe the content of the edge index matrix and how it relates to the adjacency matrix."]},{"cell_type":"markdown","metadata":{"id":"6O29ZREuOwS1"},"source":["**Your answer here:**\n","\n","**Answer Nikos**\n","\n","The edge index matrix $M$ is essentially a sparse representation of the adjacency matrix. Each column $e$ corresponds to a directed edge $(i,j)$ and $e=[i,j]^T$. Thus, $M$ contains the indices of the non-zero elements of the adjacency matrix. In our case, we have undirected edges and thus the columns of $M$ come in pairs (every undirected edge is replaced by its two directed counterparts).\n","\n","**Answer Aristotelis**\n","\n","The edge index matrix lists pairs of connected nodes, with each column representing an edge (source and destination nodes). This is essentially a sparse representation of the adjacency matrix."]},{"cell_type":"markdown","metadata":{"id":"Z0VxhCEZlj3A"},"source":["Now, we will create two random binary masks on the nodes: one for training and one for testing. We would like to have 70% of the samples in the training split, so we will uniformly pick nodes with that probability.\n","\n","We use a masking strategy instead of directly splitting the data because our interpretation of the task is that we have a social network in which the training label are accessible, while the test nodes, even though available, are unknown. This simplifies the sampling strategy, in particular for network methods, as we do not have to worry about loosing structure."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":327,"status":"ok","timestamp":1713361680916,"user":{"displayName":"Nikolaos Efthymiou","userId":"04824858864620672700"},"user_tz":-120},"id":"Ci3Hoxz-ljdU","outputId":"56f388e0-d0e9-4fbf-dc9d-7a8c7e745f93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size: 29636 (78.61%)\n","Test set size: 8064 (21.39%)\n","Ratio of class 1 in training: 74.12%\n"]}],"source":["rng = torch.Generator().manual_seed(452)\n","train_mask = torch.randn(n_nodes, generator=rng) < 0.8\n","\n","n_nodes_tr = train_mask.sum().item()\n","print(f\"Training set size: {n_nodes_tr} ({n_nodes_tr / n_nodes:.2%})\")\n","print(f\"Test set size: {n_nodes - n_nodes_tr} ({1 - n_nodes_tr / n_nodes:.2%})\")\n","print(\n","    f\"Ratio of class 1 in training: {torch.sum(train_mask * data.y).item() / n_nodes_tr:.2%}\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"m0hIHBZWO_E2"},"source":["We saw that the graph has 37,700 nodes, which means that the dense adjacency matrix has 1,421,290,000 entries.\n","Supposing that binary variables are stored in a single bit, this would still require ~170 MB to store. With 8bit integers or floats it would occupy more than 10 GB, but with mainly zero values.\n","\n","Since each node can fit in 16 bits, this representation can fit in 2.2 MB."]},{"cell_type":"markdown","metadata":{"id":"-TTeSCNpAhNL"},"source":["## Part 1: Deep learning on graph data [45 points]\n","\n","This part presents a general workflow for deep learning and our recommended libraries: PyTorch and PyTorch Geometric.\n","We will start with classical ML baselines, to get some robust results to which we can compare. Then we will introduce Network features, to see whether they can help in our task. Finally, we start working with deep learning, and graph neural networks."]},{"cell_type":"markdown","metadata":{"id":"7kyFm7htUGtx"},"source":["### Question 1.1: Our first baseline (4 points)\n","\n","In this question we define a baseline model with a \"classical\" ml method, namely a random forest, to get an idea of what performances we can expect from the following models.\n","This model will only use node features, so it does not leverage at all the graph structure."]},{"cell_type":"markdown","metadata":{"id":"GutJnum0js4K"},"source":["**1.1.1 [2pts]** Train a random forest classifier based on the node features. Make sure to use the provided `train_mask` for both the features and the target labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtxCOzQgjsgV"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","X_train = data.x[train_mask]\n","y_train = data.y[train_mask]\n","\n","X_test = data.x[~train_mask]\n","y_test = data.y[~train_mask]\n","\n","rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n","rf_classifier.fit(X_train, y_train)\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"fu-xUZHUkyhZ"},"source":["**1.1.2 [1pts]** Predict the labels of the test nodes, then print the `classification_report`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"09E56x7SkyR6"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","y_pred = rf_classifier.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"ttmPz_DUlG4R"},"source":["**1.1.3 [1pt]** Discuss which of the metrics is the most informative one for our setting."]},{"cell_type":"markdown","metadata":{"id":"mFUAHSG_lGcH"},"source":["**Your answer here:**\n","\n","The most informative metric in this context is the F1-score since we are dealing with imbalanced data (class 0: 27961, class 1: 9739). The F1-score is good since it balances both precision and recall through a harmonic mean $\\left(\\text{hmean}(a,b) = \\frac{a\\cdot b}{a + b}\\right)$, ensuring that false positives and false negatives are taken into consideration."]},{"cell_type":"markdown","metadata":{"id":"pxBcITtgLRXQ"},"source":["### Question 1.2: Graph baseline - Laplacian eigenmaps (5 points)\n","\n","Now, let's implement a second benchmark, this time relying on structural properties. We would like to use eigenmaps of the Laplacian, but if you try to do it you would quickly run out of memory! (Go ahead and try if you will 😉)\n","\n","The adjacency matrix is too big to use it in computations, but it would be mainly filled with zeros. We can optimize memory and running time by using a **sparse representation**."]},{"cell_type":"markdown","metadata":{"id":"Ut_5nLpJW8Fy"},"source":["**1.2.1 [1pts]** Compute the Laplacian matrix as a [sparse SciPy array][scipy_sparse]. Start by creating a sparse adjacency matrix form the `edge_index`, supposing that all edge weights are 1.\n","\n","[scipy_sparse]: https://docs.scipy.org/doc/scipy/reference/sparse.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUkPozOLXsU2"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","adjacency = pyg.utils.to_scipy_sparse_matrix(data.edge_index).tocsr()\n","\n","degrees = adjacency.sum(axis=1).A.flatten()\n","degree_matrix = sparse.diags(degrees)\n","laplacian = degree_matrix - adjacency\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"FpcX9YzpdHhg"},"source":["**1.2.2 [1pts]** Use SciPy sparse linear algebra capabilities to compute the first 5 nontrivial eigenvectors of the Laplacian.\n","\n","*Note: This takes ~15 minutes, so you can change the condition to False after completing Question 1.2 to iterate more quickly over the following ones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPl0GpFNXtqb"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","if True:  # Change to True to run cell\n","\n","    eigenvalues, eigenvectors = sparse.linalg.eigsh(laplacian, k=6, which='SM')\n","    eigvecs = eigenvectors[:, 1:6] # The first one is the trivial eigenvale, eigenvector pair\n","\n","    for i in range(6):\n","        print(f\"{eigenvalues[i] = }, {eigenvectors[:, i] = }\")\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","else:\n","    eigvecs = np.random.rand(n_nodes, 5)"]},{"cell_type":"markdown","metadata":{"id":"YSA1eROhf6C0"},"source":["**1.2.3 [1pts]** Train and test a new random forest classifier using the eigenvector representation as features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9auF0OKHsQ6"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","X_features = eigvecs\n","labels = data.y.numpy()\n","\n","X_train = X_features[train_mask]\n","y_train = labels[train_mask]\n","X_test = X_features[~train_mask]\n","y_test = labels[~train_mask]\n","\n","rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n","rf_classifier.fit(X_train, y_train)\n","\n","y_pred = rf_classifier.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"WLmBT9l0IUh-"},"source":["**1.2.4 [1pts]** Now, combine the two sets of features, i.e. the given programmers features and the Laplacian eigenmaps, into a single design matrix, then train and test another RF."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hS6Wk8rf769"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","combined_features = np.hstack((data.x.numpy(), eigvecs))\n","\n","X_train = combined_features[train_mask]\n","y_train = labels[train_mask]\n","X_test = combined_features[~train_mask]\n","y_test = labels[~train_mask]\n","\n","rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n","rf_classifier.fit(X_train, y_train)\n","\n","y_pred = rf_classifier.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"bxSH_KELJHTh"},"source":["**1.2.5 [1pts]** Comment on the results and describe which model you expect to perform the best on unseen dataset.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"StE6dXwWXtxP"},"source":["**Your answer here:**\n","\n","**First Baseline (Node based):**\n","\n","High accuracy, good precision and recall for class 1, but lower for class 0.\n","\n","$\\rightarrow$ Difficulty identifying the minority class.\n","\n","**Second Baseline (L Eigenvectors based):**\n","\n","Loewr accuracy, high recall for class 1, but very low for class 0.\n","\n","$\\rightarrow$ Difficulty identifying the minority class.\n","\n","**Third Baseline (Combination):**\n","\n","High accuracy, improvement in balance of the recall between the two classes.\n","\n","$\\rightarrow$ Better generalization.\n","\n","**Conclusion**\n","\n","We can conclude that the **third model** which generalizes well across both classes is expected to perform better on unseen data."]},{"cell_type":"markdown","metadata":{"id":"LRXIUrGNWpKe"},"source":["### Question 1.3: Neural Network baseline - MLP (16 points)\n","\n","In this question, we move from classical ML to deep learning and, again, we start from a simple model to get a viable benchmark."]},{"cell_type":"markdown","metadata":{"id":"lPBDUDIM-O2E"},"source":["**1.3.1 [2pts]** Create two `DataLoaders`, for the training and test data respectively, by using the `TensorDataset` class. Use the predefined batch size and shuffle training data.\n","\n","References:\n","- [Datasets and DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n","- [PyTorch data utility](https://pytorch.org/docs/stable/data.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zH3G5rTBtp6D"},"outputs":[],"source":["batch_size = 128\n","\n","# Your solution here ###########################################################\n","\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n","\n","# SHOULD WE NOT USE THE COMBINED FEATURES?\n","#  Then use:\n","#   data.x[train_mask] and data.y[train_mask] for training\n","#   data.x[~train_mask] and data.y[~train_mask] for testing\n","\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"5aIHAOnGL9Sm"},"source":["**1.3.2 [2pts]** Define a [torch module][nn_module] for a two-layer perceptron with the ReLU activation function. The hidden dimension will be a parameter of the constructor function.\n","This neural network will take as input a design matrix and predict the \"logits\" of class 1.\n","\n","[nn_module]: https://pytorch.org/docs/stable/generated/torch.nn.Module.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HiyHwfHLgsIT"},"outputs":[],"source":["class MLP(nn.Module):\n","    # Your solution here #######################################################\n","    def __init__(self, in_features: int, hidden_features: int):\n","      super(MLP, self).__init__()\n","      self.layer1 = nn.Linear(in_features, hidden_features)\n","      self.relu = nn.ReLU()\n","      self.layer2 = nn.Linear(hidden_features, 2)\n","\n","    def forward(self, x):\n","      x = self.layer1(x)\n","      x = self.relu(x)\n","      x = self.layer2(x)\n","      return x\n","    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"fDJDlVGLtoTp"},"source":["**1.3.3 [1pts]** Define a function to perform one training step for a given NN, taking as argument a batch of data `x` with target `y`, an [optimizer](torch_optim), and a [loss function](torch_loss). The function should return the loss value, as a float."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQu7n7_itgW9"},"outputs":[],"source":["def train_nn_step(\n","    optimizer: optim.Optimizer,\n","    loss_fn: nn.Module,\n","    model: nn.Module,\n","    x: torch.Tensor,\n","    y: torch.Tensor,\n","    # My parameters #\n","    is_gcn: bool=False,\n","    edge_idx=None\n","    #################\n",") -> float:\n","    model.train()  # Used to ensure that relevant blocks are in training mode\n","\n","    # Your solution here #######################################################\n","    optimizer.zero_grad()\n","\n","    if is_gcn:\n","      y_pred = model(x, edge_idx)\n","    else:\n","      y_pred = model(x)\n","\n","    loss = loss_fn(y_pred, y)\n","\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    return loss.item()\n","    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"rVzRfczBgXGj"},"source":["**1.3.4 [2pts]** Write an evaluation function that takes as input a PyTorch Module, a data loader, and a [TorchMetrics function][torchmetrics] and returns the cumulative metric over all batches.\n","\n","[TorchMetrics][torchmetrics] is a convenient package that implements metrics that work with PyTorch Tensors, and also with batched data.\n","\n","[torchmetrics]: https://lightning.ai/docs/torchmetrics/stable/pages/quickstart.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_P-metq4DA8E"},"outputs":[],"source":["def eval_nn(model: nn.Module, loader: DataLoader, metric_fn: Metric) -> float:\n","    model.eval()  # Used to ensure that relevant block are in evaluation model\n","\n","    # Your solution here #######################################################\n","    with torch.no_grad():\n","      for sample in loader:\n","        input, label = sample\n","        input.to(device)\n","        label.to(device)\n","\n","        output = model(input.to(device))\n","        _, pred = torch.max(output, dim=1)\n","\n","        metric_fn(pred.to(device), label.to(device))\n","\n","    metric = metric_fn.compute()\n","\n","    metric_fn.reset()\n","\n","    return metric\n","    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"ctd5U6RH0yHY"},"source":["**1.3.5 [2pts]** Create an instance of the previously defined MLP, a relevant [loss function][torch_loss] for our classification task, and an [optimizer](torch_optim).\n","When needed, as for the MLP hidden dimension and optimizer learning rate, select parameters that provide good results for the task. You might need some trial-and-error, so keep track of you results as you will be asked to comment on those hyperparameters.\n","\n","Make sure to send everything to the correct device at initialization, as moving information from the CPU to the GPU is time-consuming.\n","To use GPUs, you might have to change runtime type.\n","\n","[torch_loss]: https://pytorch.org/docs/stable/nn.html#loss-functions\n","[torch_optim]: https://pytorch.org/docs/stable/optim.html\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-H7iUiLHtp2K"},"outputs":[],"source":["print(f\"Using {device} device\")\n","\n","# Your solution here ###########################################################\n","\n","in_features = X_train.shape[1] # n_feats\n","hidden_features = 256   # Optimal values\n","learning_rate = 1e-4    # Optimal values\n","\n","mlp = MLP(in_features=in_features, hidden_features=hidden_features).to(device)\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)\n","\n","optimizer = optim.Adam(mlp.parameters(), lr=learning_rate)\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"EG1gxH1xDBw0"},"source":["**1.3.6 [3pts]** Perform 10 epochs of training. During each epoch, you should perform training steps iterating over the whole dataset. Gather the losses of each batch, and plot the evolution of the training loss at the end."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCmbnke5tqAq"},"outputs":[],"source":["n_epochs = 15\n","\n","# Your solution here ###########################################################\n","\n","all_losses = []\n","\n","for epoch in range(n_epochs):\n","  epoch_losses = []\n","\n","  for batch in loader_train:\n","    optimizer.zero_grad\n","    input, label = batch\n","\n","    loss = train_nn_step(optimizer, loss_fn, mlp, input.to(device), label.to(device))\n","    epoch_losses.append(loss)\n","  all_losses.extend(epoch_losses)\n","  print(f\"Epoch {epoch+1}/{n_epochs}, Average Loss: {sum(epoch_losses)/len(epoch_losses)}\")\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(all_losses, label='Training Loss')\n","plt.xlabel('Batch Number')\n","plt.ylabel('Loss')\n","plt.title('Training Loss Over Time')\n","plt.legend()\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"180-fQrevQ64"},"source":["**1.3.7 [1pts]** Evaluate the trained model on both the training and test data, using the most relevant metric from those already imported from TorchMetrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JETfZaKld3dl"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","metric_fn = BinaryF1Score().to(device)\n","\n","metric_tr = eval_nn(mlp, loader_train, metric_fn)\n","metric_te = eval_nn(mlp, loader_test, metric_fn)\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","print(f\"Training metric: {metric_tr:.3f}\")\n","print(f\"Test metric:     {metric_te:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"csB2WeikeKHD"},"source":["**1.3.8 [3pts]** Try different hyperparameters' combinations, in particular for the hidden dimension of the MLP and the learning rate of the optimizer. Then discuss the obtained results and the learning curves."]},{"cell_type":"markdown","metadata":{"id":"wcX8v2A0OlhQ"},"source":["**Your answer here:**\n","\n","**Your answer here:**\n","\n","In order to finetune the hyperparameters (the learning rate and the number of hidden features), a (manual) grid search was performed, yielding the following results (F1-scores):\n","\n","**Train**\n","\n","| lr/h | 32    | 64    | 128   | 256   | 512   |\n","|------|-------|-------|-------|-------|-------|\n","| **1e-3** | 0.922 | 0.935 | 0.951 | 0.960 | 0.972 |\n","| **1e-4** | 0.910 | 0.910 | 0.912 | **0.914** | 0.918 |\n","| **1e-5** | 0.893 | 0.892 | 0.899 | 0.903 | 0.905 |\n","\n","**Test**\n","\n","| lr/h | 32    | 64    | 128   | 256   | 512   |\n","|------|-------|-------|-------|-------|-------|\n","| **1e-3** | 0.905 | 0.901 | 0.892 | 0.899 | 0.887 |\n","| **1e-4** | 0.910 | 0.910 | 0.909 | **0.911** | 0.909 |\n","| **1e-5** | 0.896 | 0.894 | 0.905 | 0.906 | 0.907 |\n","\n","Looking at these tables we can clearly see that the optimal pair is ($lr=\\text{1e-4},~h = 256$). For a learning rate of 1e-3 we can see a clear overfit especially when the number of hidden features increases and the complexity of the model increases."]},{"cell_type":"markdown","metadata":{"id":"hIJUsBUzO3pU"},"source":["### Question 1.4: Graph Neural Networks (20 points)\n","\n","We will now shift from the standard deep learning paradigm to Graph Neural Networks, to leverage the additional structure of our data.\n","\n","We already imported [PyTorch Geometric][torch_geometric] as `pyg`, so you can access its submodules as `pyg.nn`, `pyg.data` and so on.\n","\n","[torch_geometric]: https://pytorch-geometric.readthedocs.io/en/latest/index.html"]},{"cell_type":"markdown","metadata":{"id":"F0decSDJ6Cag"},"source":["**1.4.1 [2pts]** Let's start by defining our first GNN. Again, it will be a subclass of the PyTorch `Module`, but this time it will take into account the `edge_index` in its `forward method`. Use two [GCN layers][gcn] to go from input features, here called *channels*, to a hidden dimension defined in the constructor, then to logit readout. Use ReLU activations.\n","\n","This GNN will map node vectors to node logits, so we can directly read out node probabilities from\n","\n","[gcn]: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhIU33lY6H-Q"},"outputs":[],"source":["class GCN(nn.Module):\n","    # Your solution here #######################################################\n","    def __init__(self, in_channels: int, hidden_channels: int):\n","      super(GCN, self).__init__()\n","      self.conv = pyg.nn.GCNConv(in_channels, hidden_channels)\n","      self.linear = nn.Linear(hidden_channels, 2)\n","\n","    def forward(self, x, edge_index):\n","      x = self.conv(x, edge_index).relu()\n","      x = self.linear(x)\n","\n","      return x\n","    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"BwZEvTJlHbKb"},"source":["**1.4.2 [3pts]** Perform `n_epochs` of training of a GCN model with 64 hidden channels, using full training data as a batch. Make sure to only use training data in the loss computation by using the `train_mask`. Track the loss value at each step and plot it. Finally, evaluate the model on train and test, using the `metric_fn` from before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pllLi-s9H79q"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","in_features = n_feats\n","hidden_features = 64\n","learning_rate = 1e-3\n","\n","gcn = GCN(in_channels=in_features, hidden_channels=hidden_features).to(device)\n","\n","optimizer = torch.optim.Adam(gcn.parameters(), lr=learning_rate)\n","loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","edge_index = data.edge_index.to(device)\n","\n","n_epochs = 100\n","loss_values = []\n","\n","for epoch in range(n_epochs):\n","  optimizer.zero_grad()\n","\n","  out = gcn(data.x.to(device), data.edge_index.to(device))\n","\n","  loss = loss_fn(out[train_mask].to(device), data.y[train_mask].to(device))\n","\n","  loss.backward()\n","  optimizer.step()\n","\n","  loss_values.append(loss.item())\n","  if epoch % 9 == 0:\n","    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}\")\n","\n","def evaluate(model, features, edge_index, mask, metric_fn):\n","    model.eval()\n","    with torch.no_grad():\n","        preds = model(features, edge_index).argmax(dim=-1)\n","        preds = preds[mask]\n","        labels = data.y[mask].to(device)\n","        metric_fn(preds.to(device), labels.to(device))\n","        metric_value = metric_fn.compute()\n","        metric_fn.reset()\n","        return metric_value\n","\n","metric_fn = BinaryF1Score().to(device)\n","metric_tr = evaluate(gcn, data.x.to(device), data.edge_index.to(device), train_mask, metric_fn)\n","metric_te = evaluate(gcn, data.x.to(device), data.edge_index.to(device), ~train_mask, metric_fn)\n","\n","plt.plot(loss_values)\n","plt.title('Training Loss Over Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.grid(True)\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","plt.show()\n","\n","print(f\"Training metric: {metric_tr:.3f}\")\n","print(f\"Test metric:     {metric_te:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"ceMxKgYzOmGI"},"source":["**1.4.3 [2pts]** Hopefully, we got already some good results, but we would like to test whether stochastic optimization might be better. Batching graph data requires a particular approach, since on top of the design matrix with node features we have to account for edge information. In our setting, we have a single graph with many nodes, and a node level task. A batching strategy consists in sampling nodes with their neighbors, then working with this smaller graphs in a batched way.\n","\n","Define one [NeighborLoader][neighborloader] for the training data, which will gather neighbors for as many *iterations* as layers in your GCN.\n","\n","References:\n","- [Mini batches](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#mini-batches)\n","\n","[neighborloader]: https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbtiFv4a4aPW"},"outputs":[],"source":["batch_size = 1024\n","\n","# Your solution here ###########################################################\n","\n","loader_graph_train = pyg.loader.NeighborLoader(\n","    data,\n","    input_nodes=train_mask,\n","    num_neighbors=[30]*2,\n","    batch_size=batch_size,\n","    shuffle=True,\n",")\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"g18kADvUSJV2"},"source":["**1.4.4 [1pt]** Use the previously defined `train_nn_step` to train a newly initialized GCN with the new loader. Again plot the loss evolution and evaluate the trained model on train and test data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A127T8Xld3Zz"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","in_features = n_feats\n","hidden_features = 64\n","learning_rate = 1e-3\n","\n","gcn = GCN(in_channels=in_features, hidden_channels=hidden_features).to(device)\n","optimizer = torch.optim.Adam(gcn.parameters(), lr=learning_rate)\n","loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","\n","n_epochs = 100\n","all_losses = []\n","for epoch in range(n_epochs):\n","  epoch_loss = 0\n","  for batch in loader_graph_train:\n","    loss = train_nn_step(optimizer, loss_fn, gcn, batch.x.to(device), batch.y.to(device), is_gcn=True, edge_idx=batch.edge_index.to(device))\n","    epoch_loss += loss\n","  avg_loss = epoch_loss / len(loader_train)\n","  all_losses.append(avg_loss)\n","  if (epoch + 1) % 10 == 0:\n","    print(f\"Epoch {epoch+1}/{n_epochs}, Average Loss: {avg_loss:.4f}\")\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(all_losses, label='Training Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Loss Evolution During Training')\n","plt.legend()\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"a1ymNskzZ2Mx"},"source":["**1.4.5 [1pts]** Predict the label probabilities of each node and evaluate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCApzHuVd3ja"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","def evaluate(model, loader, metric_fn):\n","  model.eval()\n","  metric_fn.reset()\n","  with torch.no_grad():\n","    for batch in loader:\n","      batch.to(device)\n","      preds = model(batch.x, batch.edge_index).argmax(dim=1)\n","      metric_fn.update(preds, batch.y)\n","  return metric_fn.compute()\n","\n","loader_graph_test = pyg.loader.NeighborLoader(\n","    data,\n","    num_neighbors=[30]*2,\n","    input_nodes=~train_mask,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")\n","\n","metric_fn = BinaryF1Score().to(device)\n","metric_tr = evaluate(gcn, loader_graph_train, metric_fn)\n","metric_te = evaluate(gcn, loader_graph_test, metric_fn)\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","print(f\"Training metric: {metric_tr:.3f}\")\n","print(f\"Test metric:     {metric_te:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"tfw4YpuYbJwT"},"source":["**1.4.6 [2pts]** Define a new GNN architecture using [graph attention layers][gat].\n","\n","[gat]: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATv2Conv.html#torch_geometric.nn.conv.GATv2Conv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgMuSdBgbmHP"},"outputs":[],"source":["class GAT(nn.Module):\n","    # Your solution here #######################################################\n","    def __init__(self, in_channels: int, hidden_channels: int):\n","      super(GAT, self).__init__()\n","      self.conv = pyg.nn.GATv2Conv(in_channels, hidden_channels)\n","      self.linear = nn.Linear(hidden_channels, 2)\n","\n","    def forward(self, x, edge_index):\n","      x = self.conv(x, edge_index).relu()\n","      x = self.linear(x)\n","\n","      return x\n","    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"RlTRMkPYb2zc"},"source":["**1.4.7 [3pts]** Train and evaluate the GAT model with both methods."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def full_training(model, optim, loss_fn, n_epochs, verbose=False):\n","\n","  loss_values = []\n","\n","  for epoch in range(n_epochs):\n","    optim.zero_grad()\n","\n","    out = model(data.x.to(device), data.edge_index.to(device))\n","\n","    loss = loss_fn(out[train_mask].to(device), data.y[train_mask].to(device))\n","\n","    loss.backward()\n","    optim.step()\n","\n","    loss_values.append(loss.item())\n","\n","    if (epoch + 1) % 10 == 0 and verbose:\n","      print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}\")\n","\n","  return loss_values\n","\n","def batch_training(model, optim, loss_fn, n_epochs, verbose=False):\n","\n","  loss_values = []\n","\n","  for epoch in range(n_epochs):\n","    epoch_loss = 0\n","    for batch in loader_graph_train:\n","      loss = train_nn_step(optim, loss_fn, model, batch.x.to(device), batch.y.to(device), is_gcn=True, edge_idx=batch.edge_index.to(device))\n","      epoch_loss += loss\n","    avg_loss = epoch_loss / len(loader_train)\n","    loss_values.append(avg_loss)\n","    if (epoch + 1) % 10 == 0 and verbose:\n","      print(f\"Epoch {epoch+1}/{n_epochs}, Average Loss: {avg_loss:.4f}\")\n","\n","    return loss_values\n","\n","def evaluate_ft(model, features, edge_index, mask, metric_fn):\n","    model.eval()\n","    with torch.no_grad():\n","        preds = model(features, edge_index).argmax(dim=-1)\n","        preds = preds[mask]\n","        labels = data.y[mask].to(device)\n","        metric_fn(preds, labels)\n","        metric_value = metric_fn.compute()\n","        metric_fn.reset()\n","        return metric_value\n","\n","\n","def evaluate_bt(model, loader, metric_fn):\n","  model.eval()\n","  metric_fn.reset()\n","  with torch.no_grad():\n","    for batch in loader:\n","      batch.to(device)\n","      preds = model(batch.x, batch.edge_index).argmax(dim=1)\n","      metric_fn.update(preds, batch.y)\n","  return metric_fn.compute()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzVw3nEAX83K"},"outputs":[],"source":["print(\"FULL TRAINING\")\n","\n","# Your solution here ###########################################################\n","\n","in_features = n_feats\n","hidden_features = 128\n","learning_rate = 1e-3\n","n_epochs = 100\n","\n","gat = GAT(in_channels=in_features, hidden_channels=hidden_features).to(device)\n","optimizer = torch.optim.Adam(gat.parameters(), lr=learning_rate)\n","loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","\n","loss_values_ft = full_training(\n","    model=gat,\n","    optim=optimizer,\n","    loss_fn=loss_fn,\n","    n_epochs=n_epochs,\n",")\n","\n","metric_fn = BinaryF1Score().to(device)\n","metric_tr = evaluate_ft(gat, data.x.to(device), data.edge_index.to(device), train_mask, metric_fn)\n","metric_te = evaluate_ft(gat, data.x.to(device), data.edge_index.to(device), ~train_mask, metric_fn)\n","\n","plt.plot(loss_values_ft)\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.grid(True)\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","plt.title(\"Full training\")\n","plt.show()\n","\n","print(f\"Training metric: {metric_tr:.3f}\")\n","print(f\"Test metric:     {metric_te:.3f}\")\n","\n","print(\"BATCH TRAINING\")\n","\n","# Your solution here ###########################################################\n","\n","in_features = n_feats\n","hidden_features = 512\n","learning_rate = 1e-3\n","n_epochs = 100\n","\n","gat = GAT(in_channels=in_features, hidden_channels=hidden_features).to(device)\n","optimizer = torch.optim.Adam(gat.parameters(), lr=learning_rate)\n","loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","\n","loss_values_bt = full_training(\n","    model=gat,\n","    optim=optimizer,\n","    loss_fn=loss_fn,\n","    n_epochs=n_epochs,\n",")\n","\n","metric_fn = BinaryF1Score().to(device)\n","metric_tr = evaluate_bt(gat, loader_graph_train, metric_fn)\n","metric_te = evaluate_bt(gat, loader_graph_test, metric_fn)\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(all_losses, label='Training Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","plt.title(\"Full training\")\n","plt.show()\n","\n","print(f\"Training metric: {metric_tr:.3f}\")\n","print(f\"Test metric:     {metric_te:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"VM66VoRPUEdd"},"source":["**1.4.8 [6pts]** Compare the results of these two architectures, with multiple hyperparameters, and the previous results. Discuss the eventual differences in performance highlighting the properties that you believe influence most the outcome."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Runtime: ~10min with GPU\n","import itertools\n","\n","print(\"FULL TRAINING\")\n","\n","in_features = n_feats\n","\n","h_list = [64, 128, 256, 512]\n","lr_list = [1e-4, 1e-3, 1e-2]\n","n_epochs = [50, 100, 150]\n","\n","train_f1_ft = {}\n","test_f1_ft = {}\n","\n","for n, lr, h in itertools.product(n_epochs, lr_list, h_list):\n","  print(f'({n = }, {lr = }, {h = })')\n","  gat = GAT(in_channels=in_features, hidden_channels=h).to(device)\n","  optimizer = torch.optim.Adam(gat.parameters(), lr=lr)\n","  loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","\n","  loss_values_ft = full_training(\n","      model=gat,\n","      optim=optimizer,\n","      loss_fn=loss_fn,\n","      n_epochs=n,\n","      verbose=False,\n","  )\n","\n","  metric_fn = BinaryF1Score().to(device)\n","  metric_tr = evaluate_ft(gat, data.x.to(device), data.edge_index.to(device), train_mask, metric_fn)\n","  metric_te = evaluate_ft(gat, data.x.to(device), data.edge_index.to(device), ~train_mask, metric_fn)\n","\n","  train_f1_ft[f'({n = }, {lr = }, {h = })'] = metric_tr.item()\n","  test_f1_ft[f'({n = }, {lr = }, {h = })'] = metric_te.item()\n","\n","print(\"BATCH TRAINING\")\n","\n","in_features = n_feats\n","\n","hidd_dims_list = [32, 64, 128, 256, 512]\n","lr_list = [1e-5, 1e-4, 1e-3]\n","n_epochs = [50, 100, 150]\n","\n","train_f1_bt = {}\n","test_f1_bt = {}\n","\n","for n, lr, h in itertools.product(n_epochs, lr_list, h_list):\n","  print(f'({n = }, {lr = }, {h = })')\n","  gat = GAT(in_channels=in_features, hidden_channels=h).to(device)\n","  optimizer = torch.optim.Adam(gat.parameters(), lr=lr)\n","  loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","\n","  loss_values_bt = batch_training(\n","      model=gat,\n","      optim=optimizer,\n","      loss_fn=loss_fn,\n","      n_epochs=n,\n","      verbose=False,\n","  )\n","\n","  metric_fn = BinaryF1Score().to(device)\n","  metric_tr = evaluate_bt(gat, loader_graph_train, metric_fn)\n","  metric_te = evaluate_bt(gat, loader_graph_test, metric_fn)\n","\n","  train_f1_bt[f'({n = }, {lr = }, {h = })'] = metric_tr.item()\n","  test_f1_bt[f'({n = }, {lr = }, {h = })'] = metric_te.item()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_test_ft = sorted(test_f1_ft.items(), key=lambda x: x[1], reverse=True)[:5]\n","top_train_ft = sorted(train_f1_ft.items(), key=lambda x: x[1], reverse=True)[:5]\n","top_test_bt = sorted(test_f1_bt.items(), key=lambda x: x[1], reverse=True)[:5]\n","top_train_bt = sorted(train_f1_bt.items(), key=lambda x: x[1], reverse=True)[:5]\n","\n","print(\"Full Training\")\n","for i, (k, v) in enumerate(top_test_ft):\n","  print(f\"\\tTop {i+1} test full training: {k} -> {v:.3f}\")\n","\n","for i, (k, v) in enumerate(top_train_ft):\n","  print(f\"\\tTop {i+1} train full training: {k} -> {v:.3f}\")\n","\n","print(\"Batch Training\")\n","for i, (k, v) in enumerate(top_test_bt):\n","    print(f\"\\tTop {i+1} test batch training: {k} -> {v:.3f}\")\n","\n","for i, (k, v) in enumerate(top_train_bt):\n","    print(f\"\\tTop {i+1} train batch training: {k} -> {v:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"HY8d_mm-Z-sp"},"source":["**Your answer here:**\n","\n","**Your answer here:**\n","\n","Looking at the above f1-scores comparing **full training** and **batch training** for various values of their hyperparameters (n: number of epochs, h: hidden dimensions, lr: learning rate), we kept the model simple in order to facilitate the tunning (potential hyperparameters to consider: dropout rate, batch normalization, number of attention heads etc.).\n","- **Test F1-Score:** The best scores for full training and batch training are 0.9147 and 0.9172 respectively. The batch training is generalizes slightly better, but no great difference.\n","\n","- **Train F1-Score:** Full training are generally higher compared to batch training, achieving 0.9995 and 0.9177 respectively. These significantly greater scores in the training compared to the testing indicate overfitting on the train set for the full training.\n","\n","**Influence of hyperparameters**\n","- **Hidden Units:** Higher number of hidden units result in higher performances. Increased complexity, can capture more complex patterns in the data, enhancing performance. However if a model is too complex it can lead to overfitting.\n","- **Learning Rate:** All top performing models were using a learning rate of 1e-3. This learning rate seems to provide a good balance between convergence speed and stability.\n","-**Number of epochs:** Larger batch sizes lead to some of the highest F1-scores, however the score seem to peek at 100 epochs and then saturate.\n","\n","**Conclusion**\n","\n","- Batch training has better generalization.\n","- The optimal hyperparameters for each model are:\n","  - Full train: (n = 100, lr = 1e-3, h = 128)\n","  - Batch train: (n = 100, lr = 1e-3, h = 512)\n"]},{"cell_type":"markdown","metadata":{"id":"bWzbUPRwC_YE"},"source":["## Part 2: Learning graphs [20 points]\n","\n","Graph attention layers are quite interesting, since they use the local and the incoming information of a node to give different weights to each neighbor. This is conceptually similar to learning a new graph on top of the existing one.\n","\n","In this part, we design a block that, from node embeddings, will produce a new graph. The idea is similar to the one in the paper \"Discrete Graph Structure Learning for Forecasting Multiple Time Series\", which is illustrated in the following Figure.\n","\n","References:\n","- C. Shang, J. Chen, and J. Bi, “Discrete Graph Structure Learning for Forecasting Multiple Time Series,” presented at the International Conference on Learning Representations, Feb. 2022. Accessed: Aug. 15, 2022. Available: https://openreview.net/forum?id=WEHSlH5mOk\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"spfK3A4tUuP9"},"source":["![Graph Learning Module](graph_learning_module.png)"]},{"cell_type":"markdown","metadata":{"id":"LG8YFdReLEvP"},"source":["As we can see from the schema, we have three main components:\n","1. **Feature extractor**: mapping each node to a new, synthesized representation;\n","2. **Link predictor**: for each pair of node representations, predict the probability that an edge links them. We gather probabilities in a *structure matrix* $\\theta$;\n","3. **Sampling**: Sample one, or multiple, discrete graphs from the structure matrix.\n","\n","In the following questions, we will break down these components."]},{"cell_type":"markdown","metadata":{"id":"rr50GNoAVK44"},"source":["### Question 2.1: Sampling (4 points)\n","\n","Sampling is the most intriguing part of our module, as it maps, randomly, continuous probabilities to discrete edges.\n","Ideally, we would like to sample each edge with a probability $\\theta$, following a Bernoulli distribution, but this would be hard to backpropagate through.\n","\n","What we do instead is known as the **Gumbel Trick**.\n","We sample edges using a [Gumbel][gumbel] reparameterization, which allows differentiating for $\\theta$ through it. With $g_{ij}^1, g_{ij}^2 \\sim \\operatorname{Gumbel}(0,1)$ for all $i,j$, and $s$ a temperature parameter,\n","$$\n","A_{ij} = \\operatorname{sigmoid}\\left(\n","  \\frac{\n","    \\log\\left( \\frac{\\theta_{ij}}{1 - \\theta_{ij}} \\right)\n","    + g_{ij}^1 - g_{ij}^2\n","  }{s}\n","\\right)\n",".\n","$$\n","By letting the temperature go to zero, we can get closer and closer to a Bernoulli distribution.\n","\n","[gumbel]: https://en.wikipedia.org/wiki/Gumbel_distribution\n"]},{"cell_type":"markdown","metadata":{"id":"qmZ4U5FBTylP"},"source":["**2.1.1 [2pts]** Define a function to sample a matrix of Gumbel variables of given shape, knowing that, for $p$ sampled uniformly in (0,1), then $Q(p) \\sim \\operatorname{Gumbel}(\\mu,\\beta)$\n","$$\n","  Q(p)=\\mu-\\beta \\ln (-\\ln (p))\n","  .\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMPBfsrvcFA7"},"outputs":[],"source":["def sample_gumbel(shape, mu=0, beta=1):\n","    # Your solution here #######################################################\n","    ...\n","\n","    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","\n","print(\"Testing sample_gumbel\")\n","print(sample_gumbel((2, 3), 0, 1))"]},{"cell_type":"markdown","metadata":{"id":"z1c-H65hZjIj"},"source":["**2.1.2 [2pts]** Note that $log(\\frac{\\theta}{1 - \\theta})$ is the sigmoid function, so we can work with unnormalized edge logits instead of probabilities. Define a function to sample an adjacency matrix $A$ from edge logits using the Gumbel Trick."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoQk5QzOXlLM"},"outputs":[],"source":["def sample_gumbel_trick(logits, temperature, mu=0, beta=1):\n","    # Your solution here #######################################################\n","    ...\n","    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","\n","print(\"Testing sample_gumbel_trick\")\n","print(sample_gumbel_trick(torch.tensor([1000, 0, 0, -10]), temperature=10))\n","print(sample_gumbel_trick(torch.tensor([1000, 0, 0, -10]), temperature=1))\n","print(sample_gumbel_trick(torch.tensor([1000, 0, 0, -10]), temperature=1e-2))"]},{"cell_type":"markdown","metadata":{"id":"umLVURybMrBn"},"source":["### Question 2.2: Link predictor (11 points)\n","\n","GNNs are all about node embeddings, which by now we should know how to deal with. The missing component is therefore the **link predictor**.\n","\n","Naively, we could iterate through all pairs of nodes and apply a predictor layer, but it would be highly inefficient.\n","To leverage tensor manipulation, let's start by gathering paired node representations in a matrix, so that we can predict probabilities in parallel."]},{"cell_type":"markdown","metadata":{"id":"Y3Y-vdSGONQO"},"source":["**2.2.1 [3pts]**  Define a function that takes as input a tensor of node embeddings, and returns a tensor that concatenate embeddings pairwise. Use [triu_indices][triu_indices] to have pairs appearing only once and avoid self loops, and return the indices along with the embeddings.\n","\n","[triu_indices]: https://pytorch.org/docs/stable/generated/torch.triu_indices.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZLUKHp5jlXbs"},"outputs":[],"source":["def pair_embeddings(x) -> (torch.Tensor, torch.Tensor):\n","    # Your solution here ###########################################################\n","    ...\n","    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","\n","print(\"Testing pair_embeddings\")\n","print(pair_embeddings(torch.tensor([[1.0], [2.0], [3.0]]))[0])\n","print(pair_embeddings(torch.tensor([[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]]]))[0])"]},{"cell_type":"markdown","metadata":{"id":"WoAjpxhcc6No"},"source":["**2.2.2 [8pts]** Define a PyTorch module that takes as input node embeddings, compute link probabilities with a two-layer perceptron on paired embeddings, then samples edges with the Gumbel trick. The output of the forward method will be a PyTorch Geometric [EdgeIndex][edge_index] of tensors representing indices and weights corresponding to positively sampled edges. You might need a `eps` threshold to avoid numerical errors.\n","\n","[edge_index]: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.edge_index.EdgeIndex"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IH4IFTq2dSex"},"outputs":[],"source":["class MLPGraphLearn(nn.Module):\n","    # Your solution here ###########################################################\n","    def __init__(\n","        self, in_features: int, hidden_features: int, temperature: float, eps=1e-10\n","    ): ...\n","\n","    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n","\n","print(\"Testing MLPGraphLearn\")\n","mlp_gl = MLPGraphLearn(2, 5, 0.01)\n","mlp_gl(torch.tensor([[[1.0, 2.0], [0.5, 7.1], [-0.1, 0.3]]]))"]},{"cell_type":"markdown","metadata":{"id":"2_yTN8dYkign"},"source":["### Question 2.3: Classifiers with Graph Learning Module (5 points)\n","\n","Let's introduce our graph learning block into some classifiers.\n"]},{"cell_type":"markdown","metadata":{"id":"cR_4B2aCk7ZI"},"source":["**2.3.1 [4pts]** Define a classifier that first produces node embeddings with a Linear layer with ReLU activation, which it feeds to the previously defined GL module; then it performs two graph convolutions on the original node features using the learned graph."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Iix9QMkXlNk"},"outputs":[],"source":["class MLPGLClassifier(nn.Module):\n","    # Your solution here #######################################################\n","    def __init__(\n","        self,\n","        in_features: int,\n","        gl_node_features_in: int = 64,\n","        gl_node_features_hidden: int = 32,\n","        gcn_hidden: int = 64,\n","    ): ...\n","\n","    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"c9mr7BGmqZ15"},"source":["**2.3.2 [1pt]** Unfortunately, training and evaluating the `MLPGLClassifier` might take too long. Let's just test whether it works: instantiate the classifier and compute the graph embedding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QFyWI8-qiLT"},"outputs":[],"source":["# Your solution here ###########################################################\n","\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"]},{"cell_type":"markdown","metadata":{"id":"_DbKoBYBEyHl"},"source":["## Part 3 (Theory): Forcing causal structure in learned graphs [35 points]\n","\n","Understanding and mapping the causal relationships among data variables, represented by directed acyclic graphs (DAGs), presents a significant challenge. The search space for these DAGs is combinatorial, and it scales super exponentially with the number of nodes, further complicating the task.\n","\n","Assuming causal relationships between data variables, the basic DAG learning problem is formulated as follows: Let $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ be a data matrix consisting of $n$ i.i.d. observations of the random vector $x=\\left(x_1, \\ldots, x_d\\right)$ and let $\\mathbb{D}$ denote the (discrete) space of DAGs $\\mathrm{G}=(\\mathrm{V}, \\mathrm{E})$ on $d$ nodes. Given $\\mathbf{X}$, we seek to learn a DAG $\\mathrm{G}$ represented by its adjacency matrix $A$ such that:\n","\n","$$\n","\\begin{array}{rl}\n","\\min _{A \\in \\mathbb{R}^{d \\times d}} &  F(A):= \\frac{1}{2 n}\\|\\mathbf{X}-GNN(\\mathbf{X}, A)\\|_F^2+\\lambda\\|A\\|_1 \\\\\n","\\text { subject to } &  A \\in \\mathbb{D} ,\n","\\end{array}\n","$$\n","where $\\mathbb{D}$ represents a DAG space and GNN is a graph neural network that simultaneously learns a DAG $A$ and accurately predicts an estimation of the matrix $\\mathbf{X}$. The minimization of the aforementioned problem guides us towards finding the causal graph that generates the data $\\mathbf{X}$. Although $F(A)$ is continuous, the DAG constraint $A \\in \\mathbb{D}$ remains a challenge to enforce.\n","\n","The objective is to make the aforementioned problem amenable to black-box optimization (in order to use SGD, ADAM...). We aim to replace the combinatorial acyclicity constraint $A \\in \\mathbb{D}$ with a single smooth equality constraint $h(A) = 0$. Thus, the objective of this section is to find a smooth function $h: \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}$ that will satisfy the following: $h(A)=0$ if and only if $A$ is acyclic (i.e. $A \\in \\mathbb{D}$). Furthermore, we want to ensure that $h$ and its derivatives are easy to compute."]},{"cell_type":"markdown","metadata":{"id":"BsdUhlSstMs6"},"source":["### Question 3.1: DAGness property for binary matrix with spectral radius condition (8 points)\n","\n","In this question we want to find out when a matrix $A \\in\\{0,1\\}^{d \\times d}$ corresponds to an acyclic graph.\n","\n","Suppose $A \\in\\{0,1\\}^{d \\times d}$ and $r(A)<1$, $r(A)$ is the spectral radius of the matrix $A$, and it corresponds to the largest absolute eigenvalue of $A$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nx-0T_W5gwQJ"},"source":["**3.1.1 [2pts]** Justify the convergence of $\\sum_{k=0}^{\\infty} A^k$ and show that:\n","$$\\left(I_n-A\\right)^{-1}=\\sum_{k=0}^{\\infty} A^k$$"]},{"cell_type":"markdown","metadata":{"id":"Qon9WAPNhN7e"},"source":["**Your answer here:**\n","\n","We denote $I_d$ with $I$. It suffices to show that:\n","\n","$$I = \\left(\\sum_{k \\geq 0}A^k\\right)(I -A) = (I-A)\\left(\\sum_{k \\geq 0}A^k\\right).$$\n","\n","Fix an arbitrary $m \\in \\mathbb{N}$. We have that:\n","\n","$$\\begin{cases}\n","\\sum_{0\\leq k \\leq  m}A^k=I + A + \\dots + A^m\\\\\\sum_{0 \\leq k \\leq m}A^{k+1}=A+A^2+\\dots+A^{m+1}.\n","\\end{cases}$$\n","\n","Subtracting the second from the first we get:\n","\n","$$\\sum_{0\\leq k \\leq  m}A^k-\\sum_{0 \\leq k \\leq m}A^{k+1}=I - A^{m+1},$$\n","\n","or equivalently:\n","$$\\begin{cases}\n","\\left(\\sum_{0\\leq k \\leq m}A^k\\right)(I -A)=I - A^{m+1}\\\\(I -A)\\left(\\sum_{0\\leq k \\leq m}A^k\\right)=I - A^{m+1}.\n","\\end{cases}$$\n","\n","Thus, it suffices to show that:\n","\n","$$\\lim_{m \\to ∞}A^m=0.$$\n","\n","Consider the Jordan decomposition of $A=PJP^{-1}$. It is easy to see that $A^m=PJ^mP^{-1}$. We conclude by showing that $\\lim_{m \\to ∞}J^m=0$. Note that $J^m$ is block diagonal matrix whose blocks are upper triangular matrices with $\\lambda^m$ (where $\\lambda$ is an eigenvalue of $A$) on the diagonal and elements of the form $m^{O(1)}\\lambda^{O(m)}$ above the diagonal. Thus, every element of each block goes to 0 as $m\\to ∞$ (since the spectral radius of $A$ is strictly smaller than 1)"]},{"cell_type":"markdown","metadata":{"id":"xaf2Z3ovhh6q"},"source":["**3.1.2 [6pts]** Show that $A$ is a $D A G$ if and only if $\\operatorname{tr}(I-A)^{-1}=d$"]},{"cell_type":"markdown","metadata":{"id":"_q0ZTy4caC4C"},"source":["**Your answer here:**\n","\n","For a vertex $i$, let $c_i$ denote the number of cycles starting and ending in $i$. Thus, $A \\in \\mathbb{D}$ if and only if $c_i=0, \\forall i \\in V$. By definition of $A$, we have that $c_i = \\sum_{k \\geq 1} (A^k)_{i,i}$. From the previous question, we can write:\n","\n","$$\n","\\begin{align*}\n","Tr\\left((I-A)^{-1}\\right)&=Tr\\left(\\sum_{k \\geq 0}A^k\\right) \\\\ &=Tr(I) + Tr\\left(\\sum_{k \\geq 1}A^k\\right) \\\\\n","&= d + \\sum_{i \\in [d]}\\sum_{k \\geq 1}(A^k)_{i,i}\n","\\\\&= d + \\sum_{i \\in [d]}c_i \\\\&\\geq d.\n","\\end{align*}\n","$$\n","\n","Thus, we directly get that:\n","\n","$$A \\in \\mathbb{D} \\iff \\sum_{i \\in [d]}c_i = 0 \\iff Tr\\left((I-A)^{-1}\\right)=d.$$"]},{"cell_type":"markdown","metadata":{"id":"_aa4Z0v4BAq1"},"source":["Having an assumption on $r(A) < 1$ limits the application of our results. For this reason, our objective is to generalize the previous result to binary matrices.\n"]},{"cell_type":"markdown","metadata":{"id":"aPTAFE02CmnR"},"source":["### Question 3.2: DAGness property for binary matrix (8 points)\n","\n","Suppose $A \\in\\{0,1\\}^{d \\times d}$.\n","\n","**3.2.1 [4pts]**  Prove the existence of the exponential matrix, we recall that $e^{A} = \\sum_{k=0}^{\\infty} \\frac{A^k}{k !}$\n"]},{"cell_type":"markdown","metadata":{"id":"omwjhe3xjsmB"},"source":["**Your answer here:**\n","\n","Note that $\\mathbb{R}^{d \\times d}$ is a finite-dimensional normed vector space. We use the following two facts without a proof:\n","1.   Every finite-dimensional normed space $X$ is a Banach space, which in particular means that every absolutely convergent series in $X$ is also convergent.\n","2.   Any two norms $\\|\\cdot\\|_\\alpha, \\|\\cdot\\|_\\beta$ on a finite-dimensional space $X$ are topologically equivalent, i.e. $$\\exists c_1, c_2 \\in \\mathbb{R}_{>0}: c_1 \\|v\\|_\\alpha\\leq \\|v\\|_\\beta \\leq c_2 \\|v\\|_\\alpha, \\forall v \\in X.$$\n","\n","From the above, we know that $\\mathbb{R}^{d \\times d}$ is a Banach space, which in particular implies that for any matrix norm $\\|\\cdot\\|$ we have:\n","$$\\sum_{k \\geq 0}\\left\\|\\frac{A^k}{k!}\\right\\| < ∞\\implies \\sum_{k \\geq 0}\\frac{A^k}{k!} < ∞.$$\n","Thus, we only need to prove the antecedent of the above implication. From fact (2) above, we are free to choose any matrix norm. We consider a sub-multiplicative matrix norm, say the Frobenius norm $\\|\\cdot\\|_F$. By the sub-multiplicativity and the absolute homogeneity of the Frobenius norm, we have that:\n","$$\\sum_{k \\geq 0}\\left\\|\\frac{A^k}{k!}\\right\\|_F\\leq \\sum_{k \\geq 0}\\frac{\\left\\|A\\right\\|^k_F}{k!} < ∞,$$\n","since $\\forall x \\in \\mathbb{R}, e^x=\\sum_{k \\geq 0}\\frac{x^k}{k!} \\in \\mathbb{R}$ and $\\left\\|A\\right\\|_F \\in \\mathbb{R}$. This concludes the proof. QED"]},{"cell_type":"markdown","metadata":{"id":"CD-NzoDOjoex"},"source":["**3.2.2 [4pts]** Show that $A$ is a $D A G$ if and only if $\\operatorname{tr} e^A=d$, where $e^A$ is the exponential matrix."]},{"cell_type":"markdown","metadata":{"id":"TidXjsZpaDmv"},"source":["**Your answer here:**\n","\n","Similarly to Question 3.1.2, we have that:\n","$$\n","\\begin{align*}\n","Tr\\left(e^A\\right)&=Tr\\left(\\sum_{k \\geq 0}\\frac{A^k}{k!}\\right) \\\\ &=Tr(I) + Tr\\left(\\sum_{k \\geq 1}\\frac{A^k}{k!}\\right) \\\\\n","&= d + \\sum_{i \\in [d]}\\sum_{k \\geq 1}\\left(\\frac{A^k}{k!}\\right)_{i,i}\n","\\\\ &\\stackrel{A \\geq 0}{\\geq} d.\n","\\end{align*}\n","$$\n","\n","Now, it's easy to see that: $$A \\in \\mathbb{D} \\iff (A^k)_{i,i} = 0, \\forall i \\in V, k \\geq 1 \\stackrel{A \\geq 0}{\\iff} \\sum_{i \\in [d]}\\sum_{k \\geq 1}\\left(\\frac{A^k}{k!}\\right)_{i,i}= 0\\iff Tr\\left(e^A\\right)=d.$$"]},{"cell_type":"markdown","metadata":{"id":"lWkv2xPtDph7"},"source":["**3.3 [5pts]** DAGness property for weighted adjacency matrices\n","\n","Suppose $A \\in \\mathbb{R}^{d \\times d}$. Knowing that the equivalence of Question 3.2 is correct for any nonnegative weighted matrix $A$, propose a function $f$ such that $A$ is a $D A G$ if and only if $\\operatorname{tr} e^{f(A)}=d$.\n"]},{"cell_type":"markdown","metadata":{"id":"2WfQ0nmvaE0H"},"source":["**Your answer here:**\n","\n","We are looking for a function $f: \\mathbb{R}^{d \\times d} \\to \\mathbb{R}^{d \\times d}_{\\geq 0}$ such that the unweighted counterparts of graphs $A$ and $f(A)$ (note that there is a bijection between node-labeled directed graphs and real square matrices) are isomorphic. The simplest way to achieve this is to square the weight of every edge, i.e. we take $f(A)=A \\circ A$ (where $\\circ$ denotes the Hadamard product). This way, we exactly preserve the edges and the non-edges of the graph $A$. Since $A \\sim f(A)$ (simply consider the identity function on $V$, here we consider any non-zero weight as being equal to 1), we get that:\n","$$A \\in \\mathbb{D} \\iff f(A) \\in \\mathbb{D} \\stackrel{f(A) \\geq 0}{\\iff} Tr(e^{f(A)})=d,$$\n","where the last equivalence was proven in the above question."]},{"cell_type":"markdown","metadata":{"id":"xfcjLTp0Kguj"},"source":["**3.4 [7pts]** Compute the gradient of $h(A) = \\operatorname{tr} e^{f(A)}-d$."]},{"cell_type":"markdown","metadata":{"id":"P-4jHQBWaFW7"},"source":["**Your answer here:**\n","\n","We need to compute: $\\nabla_{A} (Tr(e^{A \\circ A})-d)=\\nabla_{A} Tr(e^{A \\circ A})$. Set $B=A\\circ A$. We have that: $$\\nabla_{A} Tr(e^{A \\circ A})=(\\nabla_{B} Tr(e^{B})) \\circ (\\nabla_{A} (A \\circ A)).$$\n","We compute the two gradients separately:\n","1.   For the second gradient we have:\n","$$\\nabla_{A} (A \\circ A)=2A.$$\n","2.   For the first gradient, using the linearity of the Trace operator, we have that:\n","$$\n","\\begin{align*}\n","\\nabla_{B} Tr(e^{B}) &=\\nabla_{B} Tr\\left(\\sum_{k \\geq 0}\\frac{B^k}{k!}\\right)\\\\&=\\nabla_{B} Tr\\left(\\sum_{k \\geq 0}\\frac{B^k}{k!}\\right)\\\\ &=\\sum_{k \\geq 0}\\frac{\\nabla_B Tr(B^k)}{k!}\\\\ &= 0_{d} + \\sum_{k \\geq 1}\\frac{k\\cdot (B^{k-1})^T}{k!} \\\\ &= \\sum_{k \\geq 1}\\frac{(B^{k-1})^T}{(k-1)!}\\\\ &=\\sum_{k \\geq 0}\\frac{(B^{k})^T}{k!}=(e^B)^T=(e^{A \\circ A})^T,\n","\\end{align*}\n","$$\n","where we used $0_d$ to denote the $d$-by-$d$ zero matrix. Note that we have used the following fact:\n","\n","$$\\nabla_XTr(X^k)=k\\cdot (X^{k-1})^T, \\forall k \\in \\mathbb{N}, X \\in \\mathbb{R}^{n\\times n}$$\n","\n","Thus, overall we get that: $$\\nabla_{A} (Tr(e^{A \\circ A}) - d) = (e^{A \\circ A})^T \\circ 2A.$$"]},{"cell_type":"markdown","metadata":{"id":"NmFFFXiOHVDk"},"source":["**3.5 [7pts]** Other alternatives\n","\n","Suppose $A \\in \\mathbb{R}^{d \\times d}$. For the previously proposed function $f$ show that for any $\\alpha > 0 $, $A$ is a $D A G$ if and only if $\\operatorname{tr}\\left[(I+\\alpha f(A))^d\\right]-d=0.$"]},{"cell_type":"markdown","metadata":{"id":"fqTe3RAJaGbC"},"source":["**Your answer here:**\n","\n","Note that $I$ and $\\alpha(A \\circ A)$ commute with respect to multiplication. Thus, from the binomial theorem we get that: $$(I + \\alpha (A \\circ A))^d=\\sum_{0 \\leq k \\leq d}I^{d-k}(\\alpha(A \\circ A))^k=\\sum_{0 \\leq k \\leq d}(\\alpha(A \\circ A))^k.$$\n","\n","Thus, we get that:\n","$$\\begin{align*}\n","Tr\\left((I + \\alpha (A \\circ A))^d\\right)&=Tr\\left(\\sum_{0 \\leq k \\leq d}(\\alpha(A \\circ A))^k\\right) \\\\&= \\sum_{0 \\leq k \\leq d}Tr\\left((\\alpha(A \\circ A))^k\\right) \\\\ &=\\sum_{0 \\leq k \\leq d}Tr\\left((\\alpha(A \\circ A))^k\\right) \\\\ &=d + \\sum_{1 \\leq k \\leq d}\\alpha^kTr\\left((A \\circ A)^k\\right)\n","\\end{align*}.\n","$$\n","\n","Now $\\alpha >0$ and $A\\circ A \\geq 0$ imply that:\n","$$\\sum_{1 \\leq k \\leq d}\\alpha^kTr\\left((A \\circ A)^k\\right) \\geq 0$$ \n","and it's easy to see that: $$\\sum_{1 \\leq k \\leq d}\\alpha^kTr\\left((A \\circ A)^k\\right) > 0\\iff A \\notin \\mathbb{D}.$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdrlH-53ogdV"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"vscode":{"interpreter":{"hash":"1b6258ac5f9ce9aee8bc7e8f09e746bac739d42425ff8343fe5df569d2e6cb19"}}},"nbformat":4,"nbformat_minor":0}
